from typing import TypedDict, Annotated, List, Optional
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
import operator
from dotenv import load_dotenv
import os
import json
from fastapi import FastAPI, HTTPException, Header, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from together import Together
import redis
from datetime import timedelta
import hashlib
import random

# ============ ENVIRONMENT SETUP ============
load_dotenv()

TOGETHER_API_KEY = os.getenv("TOGETHER_API_KEY")
FINE_TUNED_MODEL = os.getenv(
    "FINE_TUNED_MODEL", 
    "rahmanhabeeb360_24ae/Meta-Llama-3.1-8B-Instruct-Reference-ft-manslater-4c03ddd8"
)
ROAST_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo"

REDIS_HOST = os.getenv("REDIS_HOST", "localhost")
REDIS_PORT = int(os.getenv("REDIS_PORT", 6379))
REDIS_DB = int(os.getenv("REDIS_DB", 0))
REDIS_PASSWORD = os.getenv("REDIS_PASSWORD", None)

CHAT_LIMIT_PER_DEVICE = int(os.getenv("CHAT_LIMIT_PER_DEVICE", 5))
RATE_LIMIT_TTL = int(os.getenv("RATE_LIMIT_TTL", 86400))

os.environ["OPENAI_API_KEY"] = TOGETHER_API_KEY

# ============ REDIS SETUP ============

class RedisManager:
    """Manages Redis connections and operations"""
    
    def __init__(self):
        self.client = redis.Redis(
            host=REDIS_HOST,
            port=REDIS_PORT,
            db=REDIS_DB,
            password=REDIS_PASSWORD,
            decode_responses=True,
            socket_connect_timeout=5,
            socket_timeout=5
        )
        self._test_connection()
    
    def _test_connection(self):
        try:
            self.client.ping()
            print("‚úÖ Redis connection established")
        except redis.ConnectionError as e:
            print(f"‚ùå Redis connection failed: {e}")
            raise
    
    def get_device_usage(self, device_id: str) -> int:
        key = f"rate_limit:device:{device_id}:chat_count"
        count = self.client.get(key)
        return int(count) if count else 0
    
    def increment_device_usage(self, device_id: str, ttl: int = RATE_LIMIT_TTL) -> int:
        key = f"rate_limit:device:{device_id}:chat_count"
        pipe = self.client.pipeline()
        pipe.incr(key)
        pipe.expire(key, ttl)
        results = pipe.execute()
        return results[0]
    
    def check_rate_limit(self, device_id: str, limit: int = CHAT_LIMIT_PER_DEVICE) -> dict:
        current_usage = self.get_device_usage(device_id)
        remaining = max(0, limit - current_usage)
        return {
            "allowed": current_usage < limit,
            "current_usage": current_usage,
            "limit": limit,
            "remaining": remaining
        }
    
    def reset_device_usage(self, device_id: str) -> bool:
        key = f"rate_limit:device:{device_id}:chat_count"
        return self.client.delete(key) > 0
    
    def get_device_ttl(self, device_id: str) -> int:
        key = f"rate_limit:device:{device_id}:chat_count"
        ttl = self.client.ttl(key)
        return ttl if ttl > 0 else 0
    
    def save_chat_message(self, session_id: str, role: str, content: str, ttl: int = 86400):
        key = f"chat:{session_id}:messages"
        message = {
            "role": role,
            "content": content,
            "timestamp": str(os.urandom(4).hex())
        }
        self.client.rpush(key, json.dumps(message))
        self.client.expire(key, ttl)
    
    def get_chat_history(self, session_id: str, limit: int = 20) -> List[dict]:
        key = f"chat:{session_id}:messages"
        messages = self.client.lrange(key, -limit, -1)
        return [json.loads(msg) for msg in messages]
    
    def delete_session(self, session_id: str) -> bool:
        key = f"chat:{session_id}:messages"
        return self.client.delete(key) > 0
    
    def get_all_sessions(self) -> List[str]:
        pattern = "chat:*:messages"
        keys = self.client.keys(pattern)
        return [key.split(":")[1] for key in keys]
    
    def session_exists(self, session_id: str) -> bool:
        key = f"chat:{session_id}:messages"
        return self.client.exists(key) > 0
    
    def get_translation_cache(self, text: str) -> Optional[str]:
        key = f"translation:{hash(text)}"
        cached = self.client.get(key)
        return cached if cached else None
    
    def set_translation_cache(self, text: str, translation: str, ttl: int = 3600):
        key = f"translation:{hash(text)}"
        self.client.setex(key, ttl, translation)
    
    def increment_request_count(self, endpoint: str):
        key = f"analytics:requests:{endpoint}"
        self.client.incr(key)
    
    def get_analytics(self) -> dict:
        chat_count = self.client.get("analytics:requests:chat") or 0
        translate_count = self.client.get("analytics:requests:translate") or 0
        active_sessions = len(self.get_all_sessions())
        rate_limit_keys = self.client.keys("rate_limit:device:*:chat_count")
        total_devices = len(rate_limit_keys)
        
        return {
            "chat_requests": int(chat_count),
            "translate_requests": int(translate_count),
            "active_sessions": active_sessions,
            "tracked_devices": total_devices
        }

redis_manager = RedisManager()

# ============ DEVICE IDENTIFICATION ============

def generate_device_id(request: Request, user_agent: Optional[str] = None) -> str:
    ip = request.client.host if request.client else "unknown"
    forwarded = request.headers.get("x-forwarded-for")
    if forwarded:
        ip = forwarded.split(",")[0].strip()
    
    ua = user_agent or request.headers.get("user-agent", "unknown")
    accept_lang = request.headers.get("accept-language", "")
    accept_encoding = request.headers.get("accept-encoding", "")
    
    fingerprint = f"{ip}:{ua}:{accept_lang}:{accept_encoding}"
    device_id = hashlib.sha256(fingerprint.encode()).hexdigest()[:16]
    
    return device_id

# ============ INTENT CLASSIFICATION ============

together_client = Together(api_key=TOGETHER_API_KEY)

def classify_intent(user_message: str) -> str:
    """
    Classify if user wants TRANSLATION or just CHATTING
    Returns: "translate" or "chat"
    """
    
    classification_prompt = """You are an intent classifier for ManSlater, a savage relationship advice bot.

Analyze the user's message and determine:
- **TRANSLATE**: User is asking to interpret what a woman said/texted
- **CHAT**: User is just talking to ManSlater (greetings, complaints, casual chat, follow-ups)

TRANSLATION indicators:
- Contains quotes like "she said...", 'she texted...', "her message was..."
- Asking what something means: "what does X mean?", "decode this", "translate this"
- Direct female speech: any message with female dialogue/text
- Questions about her words/actions

CHAT indicators:
- Greetings: "hey", "yo", "sup", "what's up"
- Complaints: "bro she blocked me", "she's ignoring me again", "I messed up"
- Casual conversation: "what now?", "help me out", "I'm screwed"
- Follow-up questions: "ok but what if...", "should I...", "what do I do"
- Emotional venting: "I'm so confused", "I don't get it", "why is she like this"

EXAMPLES:

Input: "She said 'I'm fine'"
Output: TRANSLATE

Input: "she texted me 'we need to talk'"
Output: TRANSLATE

Input: "What does it mean when she says 'whatever'?"
Output: TRANSLATE

Input: "Hey bro"
Output: CHAT

Input: "What now?"
Output: CHAT

Input: "She's ignoring me again"
Output: CHAT

Input: "Bro I messed up"
Output: CHAT

Input: "Should I text her?"
Output: CHAT

Input: "I'm so confused about women"
Output: CHAT

Now classify this message:
USER MESSAGE: "{user_message}"

Respond with ONLY one word: TRANSLATE or CHAT"""

    try:
        response = together_client.chat.completions.create(
            model=ROAST_MODEL,
            messages=[
                {"role": "system", "content": "You are a binary classifier. Respond with only: TRANSLATE or CHAT"},
                {"role": "user", "content": classification_prompt.format(user_message=user_message)}
            ],
            max_tokens=5,
            temperature=0.3
        )
        
        intent = response.choices[0].message.content.strip().upper()
        
        # Validate response
        if "TRANSLATE" in intent:
            return "translate"
        elif "CHAT" in intent:
            return "chat"
        else:
            # Default: if uncertain, check for quotes as fallback
            if any(q in user_message.lower() for q in ['"', "'", "she said", "she texted", "her message"]):
                return "translate"
            return "chat"
            
    except Exception as e:
        print(f"Intent classification error: {e}")
        # Safe fallback: check for translation signals
        if any(q in user_message.lower() for q in ['"', "'", "she said", "she texted", "what does", "decode", "translate"]):
            return "translate"
        return "chat"

# ============ TRANSLATION MODE (Original) ============

def generate_roast(user_question: str) -> str:
    """Generate short roast for TRANSLATION mode"""
    roast_prompt = """You are a savage relationship coach who roasts men asking what women's words mean.

CRITICAL RULES:
1. MAXIMUM 3-4 WORDS (not sentences, WORDS!)
2. Must mock them for not understanding women
3. MUST include ONE emoji: üíÄüî•üòêüôÑ
4. NO explanations - just the roast

EXAMPLES:
Input: "She said 'I'm fine'"
Output: "Are you dumb? üíÄ"

Input: "She's ignoring me"
Output: "Weak move, bro üôÑ"

Input: "What does 'whatever' mean?"
Output: "You serious? üòê"

Generate ONLY the roast (3-4 words + emoji):
{user_question}"""

    try:
        response = together_client.chat.completions.create(
            model=ROAST_MODEL,
            messages=[
                {"role": "system", "content": "Ultra-short savage roasts. Max 4 words + emoji."},
                {"role": "user", "content": roast_prompt.format(user_question=user_question)}
            ],
            max_tokens=15,
            temperature=0.9,
            top_p=0.95
        )
        
        roast = response.choices[0].message.content.strip()
        words = roast.split()
        if len(words) > 5:
            roast = ' '.join(words[:4])
        
        if not any(emoji in roast for emoji in ['üíÄ', 'üî•', 'üòê', 'üôÑ']):
            roast += " üíÄ"
            
        return roast
        
    except Exception as e:
        print(f"Error generating roast: {e}")
        return "Seriously asking this? üíÄ"

def generate_advice(user_question: str, conversation_history: List = None) -> str:
    """Generate advice using fine-tuned model"""
    messages = []
    
    if conversation_history:
        messages.extend(conversation_history[-8:])
    
    messages.append({"role": "user", "content": user_question})
    
    try:
        response = together_client.chat.completions.create(
            model=FINE_TUNED_MODEL,
            messages=messages,
            max_tokens=150,
            temperature=0.7
        )
        advice = response.choices[0].message.content.strip()
        
        if random.random() < 0.8:
            advice = advice.replace("\n", " ")
        
        return advice
        
    except Exception as e:
        print(f"Error generating advice: {e}")
        return "just say - \"Talk to me when you're ready.\""

# ============ CHAT MODE (New) ============

def generate_savage_chat_response(user_message: str, conversation_history: List = None) -> str:
    """
    Generate savage ManSlater responses for casual chat
    Mocks the user for coming back, complaining, being clueless about women
    """
    
    chat_prompt = """You are ManSlater - a brutally savage relationship coach who roasts men who don't understand women.

The user is NOT asking to translate what a woman said. They're just CHATTING with you - complaining, venting, asking for general help, or greeting you.

YOUR PERSONALITY:
- Mock them for being back AGAIN with woman problems
- Shame them for not knowing how women work
- Short, punchy responses (1-2 sentences MAX)
- Always include ONE emoji: üíÄüî•üòêüôÑüò§
- Act like you're annoyed they need your help but you'll roast them anyway

RESPONSE STYLE:
- If they greet you: "Back again crying? üíÄ What happened now?"
- If they complain: "And you're surprised? üòê You did this to yourself."
- If they ask for help: "You really need me to explain women AGAIN? üíÄ"
- If they're venting: "Bro, she's not coming back üò§ Move on."
- Keep it SHORT - no long explanations

PREVIOUS CONTEXT:
{history}

USER SAYS: "{user_message}"

YOUR SAVAGE RESPONSE (1-2 sentences max):"""

    # Format conversation history
    history_text = ""
    if conversation_history:
        for msg in conversation_history[-4:]:  # Last 4 messages for context
            role = msg.get("role", "")
            content = msg.get("content", "")
            if role == "user":
                history_text += f"User: {content}\n"
            elif role == "assistant":
                history_text += f"ManSlater: {content}\n"
    
    if not history_text:
        history_text = "[First message]"
    
    try:
        response = together_client.chat.completions.create(
            model=ROAST_MODEL,
            messages=[
                {"role": "system", "content": "You are ManSlater. Be savage, mocking, and brutally honest. Keep responses SHORT (1-2 sentences). Always include emoji."},
                {"role": "user", "content": chat_prompt.format(
                    history=history_text,
                    user_message=user_message
                )}
            ],
            max_tokens=50,
            temperature=0.9,
            top_p=0.95
        )
        
        savage_response = response.choices[0].message.content.strip()
        
        # Ensure it has emoji
        if not any(emoji in savage_response for emoji in ['üíÄ', 'üî•', 'üòê', 'üôÑ', 'üò§']):
            savage_response += " üíÄ"
        
        # Keep it short (safety check)
        sentences = savage_response.split('.')
        if len(sentences) > 2:
            savage_response = '. '.join(sentences[:2]) + '.'
            
        return savage_response
        
    except Exception as e:
        print(f"Error generating chat response: {e}")
        return "Back again with problems? üíÄ What happened now, genius?"

# ============ LANGGRAPH SETUP ============

class ConversationState(TypedDict):
    messages: Annotated[List, operator.add]

llm = ChatOpenAI(
    base_url="https://api.together.xyz/v1",
    api_key=TOGETHER_API_KEY,
    model=FINE_TUNED_MODEL,
    temperature=0.7,
    max_tokens=150
)

def response_node(state: ConversationState) -> ConversationState:
    """Route to TRANSLATION or CHAT mode based on intent"""
    messages = state.get("messages", [])
    
    # Get last user message
    user_message = ""
    for msg in reversed(messages):
        if isinstance(msg, HumanMessage):
            user_message = msg.content
            break
    
    # Classify intent
    intent = classify_intent(user_message)
    print(f"üîç Intent detected: {intent.upper()} for message: {user_message[:50]}...")
    
    # Convert history to dict format
    conversation_history = []
    for msg in messages[:-1]:
        if isinstance(msg, HumanMessage):
            conversation_history.append({"role": "user", "content": msg.content})
        elif isinstance(msg, AIMessage):
            conversation_history.append({"role": "assistant", "content": msg.content})
    
    if intent == "translate":
        # TRANSLATION MODE: Roast + Advice
        roast = generate_roast(user_message)
        advice = generate_advice(user_message, conversation_history)
        combined_response = f"{roast}\n{advice}"
    else:
        # CHAT MODE: Just savage response (no advice format)
        combined_response = generate_savage_chat_response(user_message, conversation_history)
    
    return {"messages": [AIMessage(content=combined_response)]}

def create_advisor_graph():
    workflow = StateGraph(ConversationState)
    workflow.add_node("response", response_node)
    workflow.set_entry_point("response")
    workflow.add_edge("response", END)
    
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)

class Manslater:
    def __init__(self, session_id: str):
        self.graph = create_advisor_graph()
        self.session_id = session_id
        self.thread_id = f"thread_{session_id}"
        self.config = {"configurable": {"thread_id": self.thread_id}}
    
    def load_history_from_redis(self) -> List:
        history = redis_manager.get_chat_history(self.session_id)
        messages = []
        for msg in history:
            if msg["role"] == "user":
                messages.append(HumanMessage(content=msg["content"]))
            elif msg["role"] == "assistant":
                messages.append(AIMessage(content=msg["content"]))
        return messages
    
    def send_message(self, user_message: str):
        redis_manager.save_chat_message(self.session_id, "user", user_message)
        
        history = self.load_history_from_redis()
        input_data = {"messages": history + [HumanMessage(content=user_message)]}
        
        result = self.graph.invoke(input_data, self.config)
        
        ai_response = None
        for msg in reversed(result["messages"]):
            if isinstance(msg, AIMessage):
                ai_response = msg.content
                break
        
        if not ai_response:
            ai_response = "Something wrong? üíÄ\nTalk to me."
        
        redis_manager.save_chat_message(self.session_id, "assistant", ai_response)
        
        return ai_response

# ============ FASTAPI APP ============

app = FastAPI(title="Manslater API", version="2.3.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:5173",
        "http://127.0.0.1:3000",
        "http://127.0.0.1:5173",
        "https://manslater.in",
        "https://www.manslater.in"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

active_instances = {}

# ============ MODELS ============

class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str
    rate_limit_info: dict
    intent: Optional[str] = None  # NEW: Show detected intent

class TranslateRequest(BaseModel):
    text: str

class TranslateResponse(BaseModel):
    translatedText: str
    cached: bool = False

class RateLimitInfo(BaseModel):
    device_id: str
    current_usage: int
    limit: int
    remaining: int
    ttl_seconds: int

# ============ ENDPOINTS ============

@app.get("/")
async def root():
    return {
        "message": "Welcome to Manslater API v2.3.0",
        "version": "2.3.0",
        "features": [
            "üß† Intent Classification: Auto-detect translation vs chat",
            "üí¨ Translation Mode: Roast + 'just say' advice",
            "üò§ Chat Mode: Savage banter responses",
            f"‚è±Ô∏è Rate Limit: {CHAT_LIMIT_PER_DEVICE} chats per {RATE_LIMIT_TTL//3600}h"
        ],
        "modes": {
            "translate": "User asks what woman's words mean ‚Üí Roast + Advice",
            "chat": "User just talking/venting ‚Üí Savage ManSlater roast"
        }
    }

@app.get("/health")
@app.head("/health")
async def health_check():
    try:
        redis_manager.client.ping()
        redis_status = "connected"
    except:
        redis_status = "disconnected"
    
    return {
        "status": "healthy",
        "fine_tuned_model": FINE_TUNED_MODEL,
        "roast_model": ROAST_MODEL,
        "redis": redis_status,
        "features": ["intent_classification", "dual_mode_responses"]
    }

@app.get("/rate-limit", response_model=RateLimitInfo)
async def check_rate_limit(request: Request, user_agent: Optional[str] = Header(None)):
    device_id = generate_device_id(request, user_agent)
    rate_info = redis_manager.check_rate_limit(device_id)
    ttl = redis_manager.get_device_ttl(device_id)
    
    return RateLimitInfo(
        device_id=device_id,
        current_usage=rate_info["current_usage"],
        limit=rate_info["limit"],
        remaining=rate_info["remaining"],
        ttl_seconds=ttl
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(
    request_body: ChatRequest, 
    request: Request,
    user_agent: Optional[str] = Header(None)
):
    """
    Smart chat endpoint with intent detection
    - TRANSLATE mode: "She said..." ‚Üí Roast + Advice
    - CHAT mode: "Hey bro" ‚Üí Savage banter
    """
    try:
        device_id = generate_device_id(request, user_agent)
        rate_info = redis_manager.check_rate_limit(device_id)
        
        if not rate_info["allowed"]:
            ttl = redis_manager.get_device_ttl(device_id)
            hours_remaining = ttl // 3600
            minutes_remaining = (ttl % 3600) // 60
            
            rate_limit_message = f"Rate limit hit üòê\nYou've used all {CHAT_LIMIT_PER_DEVICE} chats. Try again in {hours_remaining}h {minutes_remaining}m."
            
            session_id = request_body.session_id or f"session_{os.urandom(8).hex()}"
            redis_manager.save_chat_message(session_id, "user", request_body.message)
            redis_manager.save_chat_message(session_id, "assistant", rate_limit_message)
            
            raise HTTPException(
                status_code=429,
                detail={
                    "error": "Rate limit exceeded",
                    "message": rate_limit_message,
                    "current_usage": rate_info["current_usage"],
                    "limit": rate_info["limit"],
                    "reset_in_seconds": ttl,
                    "device_id": device_id
                }
            )
        
        redis_manager.increment_request_count("chat")
        
        session_id = request_body.session_id or f"session_{os.urandom(8).hex()}"
        
        if session_id not in active_instances:
            active_instances[session_id] = Manslater(session_id)
        
        advisor = active_instances[session_id]
        
        # Get response (intent detection happens inside)
        response_text = advisor.send_message(request_body.message)
        
        # Detect intent for response metadata
        detected_intent = classify_intent(request_body.message)
        
        new_usage = redis_manager.increment_device_usage(device_id)
        updated_rate_info = redis_manager.check_rate_limit(device_id)
        ttl = redis_manager.get_device_ttl(device_id)
        
        return ChatResponse(
            response=response_text,
            session_id=session_id,
            intent=detected_intent,  # NEW: Show what mode was used
            rate_limit_info={
                "current_usage": updated_rate_info["current_usage"],
                "limit": updated_rate_info["limit"],
                "remaining": updated_rate_info["remaining"],
                "reset_in_seconds": ttl
            }
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat error: {str(e)}")

@app.post("/translate", response_model=TranslateResponse)
async def translate_text(request: TranslateRequest):
    """Single-turn translation endpoint (NO rate limit)"""
    if not request.text:
        raise HTTPException(status_code=400, detail="No text provided")

    try:
        redis_manager.increment_request_count("translate")
        
        cached_translation = redis_manager.get_translation_cache(request.text)
        if cached_translation:
            return TranslateResponse(
                translatedText=cached_translation,
                cached=True
            )
        
        roast = generate_roast(request.text)
        advice = generate_advice(request.text)
        combined = f"{roast}\n{advice}"
        
        redis_manager.set_translation_cache(request.text, combined)
        
        return TranslateResponse(
            translatedText=combined,
            cached=False
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Translation failed: {str(e)}")

@app.delete("/session/{session_id}")
async def delete_session(session_id: str):
    deleted = redis_manager.delete_session(session_id)
    
    if session_id in active_instances:
        del active_instances[session_id]
    
    if deleted:
        return {"message": "Session deleted successfully"}
    raise HTTPException(status_code=404, detail="Session not found")

@app.get("/sessions")
async def list_sessions():
    sessions = redis_manager.get_all_sessions()
    return {
        "active_sessions": sessions,
        "total": len(sessions)
    }

@app.get("/analytics")
async def get_analytics():
    return redis_manager.get_analytics()

@app.delete("/cache/translations")
async def clear_translation_cache():
    try:
        pattern = "translation:*"
        keys = redis_manager.client.keys(pattern)
        if keys:
            redis_manager.client.delete(*keys)
        return {"message": f"Cleared {len(keys)} cached translations"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cache clear failed: {str(e)}")

@app.post("/admin/reset-device-limit")
async def reset_device_limit(
    request: Request,
    user_agent: Optional[str] = Header(None),
    admin_key: str = Header(None)
):
    ADMIN_KEY = os.getenv("ADMIN_KEY", "your-secret-admin-key")
    
    if admin_key != ADMIN_KEY:
        raise HTTPException(status_code=403, detail="Unauthorized")
    
    device_id = generate_device_id(request, user_agent)
    reset = redis_manager.reset_device_usage(device_id)
    
    if reset:
        return {"message": "Device rate limit reset successfully", "device_id": device_id}
    return {"message": "No rate limit found for device", "device_id": device_id}

# ============ RUN SERVER ============

if __name__ == "__main__":
    import uvicorn
    print("=" * 60)
    print("üöÄ Starting Manslater API v2.3.0")
    print("   üß† Intent Classification Active")
    print("=" * 60)
    print("Server: http://localhost:8000")
    print("Docs: http://localhost:8000/docs")
    print("=" * 60)
    print("\nüéØ Response Modes:")
    print("  TRANSLATE: 'She said X' ‚Üí Roast + Advice")
    print("  CHAT: 'Hey bro' ‚Üí Savage ManSlater banter")
    print("=" * 60)
    print(f"\nModels:")
    print(f"  Intent Classifier: {ROAST_MODEL}")
    print(f"  Roast Generator: {ROAST_MODEL}")
    print(f"  Advice Model: {FINE_TUNED_MODEL}")
    print("=" * 60)
    
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)